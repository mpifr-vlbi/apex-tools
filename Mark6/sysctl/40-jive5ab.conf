
# From http://www.jive.eu/~verkout/flexbuff/flexbuf.recording.txt

# 1a.) 'sysctl' parameters for UDP/network memory

# Assume max rec. data rate of 8 Gbps / network interface
# Kernel HZ (time slice) = 100
#   => if thread swapped out, must buffer
#      1/100 of 8 Gbps = 1/100 of 1 Gbyte
#      = ~10 Mbyte. With a bit of overhead
#      we could allow for 32 Mbyte.
# packet size ~= 5000 [Mark5B frames over ethernet]
#   => 32e6 bytes / 5e3 bytes/packet ~= 6e3 packets
#      round to 8192 ...
#
# MonsterBuff @16Gbps
net.core.netdev_max_backlog = 1048576

# maximum memory usage across all sockets; 192MB should be enough for now.
# only update the max value because jive5ab never relies on the default
# value but sets it explicitly
# MonsterBuff @16Gbps
net.core.rmem_max = 1073741824    

# same if we want to send at high speed: we never
# rely on the default socket write buffer size
net.core.wmem_max = 201326592

# Our recording will be done over UDP so it's important
# to tweak these settings. UDP shouldn't even *start*
# to think about memory usage until we hit the 64MB mark,
# start pressuring by 128MB and use 192MB max
# NOTE: the units of these values are system PAGES, not bytes
#       on Loonix/x86_64 pagesize seems to be 4096
#       so scale those values by that amount:
# MonsterBuff @16Gbps
net.ipv4.udp_mem = 536870912 805306368 1073741824

# 1b.) 'sysctl' parameters for dealing with directories
#      with *many* files/directories
#     ------------------------------------------

# By default Linux is quite happy to swap out inode/dentry
# caches to free up pages for _data_. This is nice if you
# expect to use the data just read/written.
#
# On FlexBuff where we expect lots of files/directories
# but the data flow is mostly unidirectional [either we're
# recording(writing) or reading] it becomes favourable
# to retain the inode/dentry cache because the data is
# not likely to be used soon after it appeared in memory
#
# So we'll teach the kernel to NOT free inode/dentry caches
# that easily such that "ls" and "find" &cet on the many
# mount points remain responsive
#
# See https://www.kernel.org/doc/Documentation/sysctl/vm.txt
#     section: vfs_cache_pressure
# (Might even go lower than 50 - lower means more willingness
#  to retain the caches)
#vfs_cache_pressure = 50   # not available on APEX Mark6 linux version

# 1c.)  Tune minimum amount of free virtual memory
#       Do not use on MiniBuff I suppose ...
#       SC: "Aaand to always have 1GB of instantly grabbable memory..."
vm.min_free_kbytes = 1048576

